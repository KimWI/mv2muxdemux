import os, sys, math, subprocess, cv2, hashlib, warnings, argparse
import numpy as np
from sklearn.cluster import KMeans
from numba import njit, prange
from PIL import Image

try:
    import torch
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

# ==========================================================
# 1. Numba JIT ê³ ì† ì—°ì‚°ë¶€
# ==========================================================
@njit(fastmath=True, cache=True)
def _get_dist_sq(c1, c2):
    return (int(c1[0]) - int(c2[0]))**2 + (int(c1[1]) - int(c2[1]))**2 + (int(c1[2]) - int(c2[2]))**2

@njit(fastmath=True, cache=True)
def _apply_dither_rgb(img_array, pal_888, mode):
    h, w = 192, 256
    temp_img = img_array.astype(np.float32)
    if mode == 0: return temp_img
        
    strength = 0.75 
    for y in range(h):
        for x in range(w):
            r = temp_img[y, x, 0]; g = temp_img[y, x, 1]; b = temp_img[y, x, 2]
            r = 255.0 if r > 255.0 else (0.0 if r < 0.0 else r)
            g = 255.0 if g > 255.0 else (0.0 if g < 0.0 else g)
            b = 255.0 if b > 255.0 else (0.0 if b < 0.0 else b)
            
            min_d = 250000.0; best_i = 1
            for p_i in range(1, 16):
                d = (r - pal_888[p_i,0])**2 + (g - pal_888[p_i,1])**2 + (b - pal_888[p_i,2])**2
                if d < min_d: min_d, best_i = d, p_i
            
            nr, ng, nb = pal_888[best_i, 0], pal_888[best_i, 1], pal_888[best_i, 2]
            er, eg, eb = (r - nr) * strength, (g - ng) * strength, (b - nb) * strength
            
            if mode == 1: # Floyd-Steinberg
                if x + 1 < w: temp_img[y, x+1, 0] += er * 0.4375; temp_img[y, x+1, 1] += eg * 0.4375; temp_img[y, x+1, 2] += eb * 0.4375
                if y + 1 < h:
                    if x > 0: temp_img[y+1, x-1, 0] += er * 0.1875; temp_img[y+1, x-1, 1] += eg * 0.1875; temp_img[y+1, x-1, 2] += eb * 0.1875
                    temp_img[y+1, x, 0] += er * 0.3125; temp_img[y+1, x, 1] += eg * 0.3125; temp_img[y+1, x, 2] += eb * 0.3125
                    if x + 1 < w: temp_img[y+1, x+1, 0] += er * 0.0625; temp_img[y+1, x+1, 1] += eg * 0.0625; temp_img[y+1, x+1, 2] += eb * 0.0625
            elif mode == 2: # JJN
                if x + 1 < w: temp_img[y, x+1, 0] += er*(7/48); temp_img[y, x+1, 1] += eg*(7/48); temp_img[y, x+1, 2] += eb*(7/48)
                if x + 2 < w: temp_img[y, x+2, 0] += er*(5/48); temp_img[y, x+2, 1] += eg*(5/48); temp_img[y, x+2, 2] += eb*(5/48)
                if y + 1 < h:
                    if x - 2 >= 0: temp_img[y+1, x-2, 0] += er*(3/48); temp_img[y+1, x-2, 1] += eg*(3/48); temp_img[y+1, x-2, 2] += eb*(3/48)
                    if x - 1 >= 0: temp_img[y+1, x-1, 0] += er*(5/48); temp_img[y+1, x-1, 1] += eg*(5/48); temp_img[y+1, x-1, 2] += eb*(5/48)
                    temp_img[y+1, x, 0] += er*(7/48); temp_img[y+1, x, 1] += eg*(7/48); temp_img[y+1, x, 2] += eb*(7/48)
                    if x + 1 < w: temp_img[y+1, x+1, 0] += er*(5/48); temp_img[y+1, x+1, 1] += eg*(5/48); temp_img[y+1, x+1, 2] += eb*(5/48)
                    if x + 2 < w: temp_img[y+1, x+2, 0] += er*(3/48); temp_img[y+1, x+2, 1] += eg*(3/48); temp_img[y+1, x+2, 2] += eb*(3/48)
                if y + 2 < h:
                    if x - 2 >= 0: temp_img[y+2, x-2, 0] += er*(1/48); temp_img[y+2, x-2, 1] += eg*(1/48); temp_img[y+2, x-2, 2] += eb*(1/48)
                    if x - 1 >= 0: temp_img[y+2, x-1, 0] += er*(3/48); temp_img[y+2, x-1, 1] += eg*(3/48); temp_img[y+2, x-1, 2] += eb*(3/48)
                    temp_img[y+2, x, 0] += er*(5/48); temp_img[y+2, x, 1] += eg*(5/48); temp_img[y+2, x, 2] += eb*(5/48)
                    if x + 1 < w: temp_img[y+2, x+1, 0] += er*(3/48); temp_img[y+2, x+1, 1] += eg*(3/48); temp_img[y+2, x+1, 2] += eb*(3/48)
                    if x + 2 < w: temp_img[y+2, x+2, 0] += er*(1/48); temp_img[y+2, x+2, 1] += eg*(1/48); temp_img[y+2, x+2, 2] += eb*(1/48)
    return temp_img

@njit(parallel=True, fastmath=True, cache=True)
def _apply_bayer_dither(img_array, spread=36.0):
    h, w = 192, 256
    bayer_4x4 = np.array([[0,8,2,10],[12,4,14,6],[3,11,1,9],[15,7,13,5]], dtype=np.float32) / 16.0 - 0.5
    temp_img = np.zeros_like(img_array, dtype=np.float32)
    for y in prange(h):
        for x in range(w):
            offset = bayer_4x4[y % 4, x % 4] * spread
            for c in range(3):
                temp_img[y, x, c] = min(max(img_array[y, x, c] + offset, 0.0), 255.0)
    return temp_img

@njit(parallel=True, fastmath=True, cache=True)
def _apply_bayer8_dither(img_array, spread=36.0):
    h, w = 192, 256
    bayer_8x8 = np.array([
        [0,32,8,40,2,34,10,42],[48,16,56,24,50,18,58,26],[12,44,4,36,14,46,6,38],[60,28,52,20,62,30,54,22],
        [3,35,11,43,1,33,9,41],[51,19,59,27,49,17,57,25],[15,47,7,39,13,45,5,37],[63,31,55,23,61,29,53,21]
    ], dtype=np.float32) / 64.0 - 0.5
    temp_img = np.zeros_like(img_array, dtype=np.float32)
    for y in prange(h):
        for x in range(w):
            offset = bayer_8x8[y % 8, x % 8] * spread
            for c in range(3):
                temp_img[y, x, c] = min(max(img_array[y, x, c] + offset, 0.0), 255.0)
    return temp_img

def _apply_bayer_dither_cuda(img_array, spread=30.0):
    h, w = 192, 256
    img_t = torch.tensor(img_array, dtype=torch.float32, device='cuda')
    bayer_4x4 = torch.tensor([[0,8,2,10],[12,4,14,6],[3,11,1,9],[15,7,13,5]], dtype=torch.float32, device='cuda') / 16.0 - 0.5
    bayer_map = bayer_4x4.repeat(h // 4, w // 4) * spread
    bayer_map = bayer_map.unsqueeze(2)
    out_t = torch.clamp(img_t + bayer_map, 0.0, 255.0)
    return out_t.cpu().numpy()

def _apply_bayer8_dither_cuda(img_array, spread=32.0):
    h, w = 192, 256
    img_t = torch.tensor(img_array, dtype=torch.float32, device='cuda')
    bayer_8x8 = torch.tensor([
        [0,32,8,40,2,34,10,42],[48,16,56,24,50,18,58,26],[12,44,4,36,14,46,6,38],[60,28,52,20,62,30,54,22],
        [3,35,11,43,1,33,9,41],[51,19,59,27,49,17,57,25],[15,47,7,39,13,45,5,37],[63,31,55,23,61,29,53,21]
    ], dtype=torch.float32, device='cuda') / 64.0 - 0.5
    bayer_map = bayer_8x8.repeat(h // 8, w // 8) * spread
    bayer_map = bayer_map.unsqueeze(2)
    out_t = torch.clamp(img_t + bayer_map, 0.0, 255.0)
    return out_t.cpu().numpy()

@njit(parallel=True, fastmath=True, cache=True)
def _encode_vram_optimal_search(img_rgb_float, pal_888):
    h, w = 192, 256
    pgt, ct = np.zeros(6144, dtype=np.uint8), np.zeros(6144, dtype=np.uint8)
    for y in prange(h):
        for cx in range(32):
            x_start = cx * 8
            block_rgb = img_rgb_float[y, x_start : x_start + 8]
            best_err = 1e12; best_fg = 1; best_bg = 1
            
            for i in range(1, 16):
                for j in range(1, i + 1):
                    err = 0.0
                    for p in range(8):
                        r, g, b = block_rgb[p]
                        r_cl = max(0.0, min(255.0, r)); g_cl = max(0.0, min(255.0, g)); b_cl = max(0.0, min(255.0, b))
                        d_i = (r_cl - pal_888[i,0])**2 + (g_cl - pal_888[i,1])**2 + (b_cl - pal_888[i,2])**2
                        d_j = (r_cl - pal_888[j,0])**2 + (g_cl - pal_888[j,1])**2 + (b_cl - pal_888[j,2])**2
                        err += d_i if d_i < d_j else d_j
                    if err < best_err:
                        best_err = err; best_fg = i; best_bg = j
            
            p_byte = 0
            for p in range(8):
                r, g, b = block_rgb[p]
                r_cl = max(0.0, min(255.0, r)); g_cl = max(0.0, min(255.0, g)); b_cl = max(0.0, min(255.0, b))
                d_fg = (r_cl - pal_888[best_fg,0])**2 + (g_cl - pal_888[best_fg,1])**2 + (b_cl - pal_888[best_fg,2])**2
                d_bg = (r_cl - pal_888[best_bg,0])**2 + (g_cl - pal_888[best_bg,1])**2 + (b_cl - pal_888[best_bg,2])**2
                if d_fg <= d_bg: p_byte |= (1 << (7 - p))
            
            off = ((y // 8) * 32 + cx) * 8 + (y % 8)
            pgt[off] = p_byte; ct[off] = (best_fg << 4) | best_bg
    return pgt, ct

def _encode_vram_optimal_search_cuda(img_rgb_float, pal_888):
    h, w = 192, 256
    
    # í…ì„œ ë³µì‚¬ ë° GPU ë¡œë“œ
    img_t = torch.tensor(img_rgb_float, dtype=torch.float32, device='cuda')
    img_t = torch.clamp(img_t, 0.0, 255.0)
    pal_t = torch.tensor(pal_888, dtype=torch.float32, device='cuda')
    
    # í…ì„œë¥¼ (ë¸”ë¡ ê°œìˆ˜=192*32, í”½ì…€ 8ê°œ, ì±„ë„ 3ê°œ)ë¡œ ì¬ë°°ì—´
    blocks = img_t.view(h, 32, 8, 3).reshape(-1, 8, 3)
    num_blocks = blocks.shape[0]

    # ê° í”½ì…€ê³¼ íŒ”ë ˆíŠ¸ ìƒì˜ 15ê°€ì§€ ìƒ‰ìƒ(1ë²ˆ ì¸ë±ìŠ¤ë¶€í„°) ì‚¬ì´ì˜ ì œê³±ê·¼ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì—°ì‚°
    # blocks: [B, 8, 1, 3] / pal_t[1:]: [1, 1, 15, 3] -> diff: [B, 8, 15, 3]
    diff = blocks.unsqueeze(2) - pal_t[1:].unsqueeze(0).unsqueeze(0)
    dist = (diff ** 2).sum(dim=-1) # [B, 8, 15] 

    # 15ê°œì˜ ì „ê²½ìƒ‰(i)ê³¼ 15ê°œì˜ ë°°ê²½ìƒ‰(j) ê°„ì˜ ëª¨ë“  ì¡°í•© (ì´ 225ê°€ì§€. iëŠ” 1~15, jëŠ” 1~i) 
    # í•˜ì§€ë§Œ ì—°ì‚°ì˜ ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ i, j 1~15 ì „ì²´ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ êµ¬ì„±í•˜ê³  GPU ë¸Œë¡œë“œìºìŠ¤íŒ… 
    d_i = dist.unsqueeze(3) # [B, 8, 15, 1] - ì „ê²½ìƒ‰ ê±°ë¦¬ 
    d_j = dist.unsqueeze(2) # [B, 8, 1, 15] - ë°°ê²½ìƒ‰ ê±°ë¦¬
    
    # í”½ì…€ë§ˆë‹¤ d_i ê°€ ì‘ì€ì§€ d_j ê°€ ì‘ì€ì§€ ì·¨í•©.
    min_dist = torch.minimum(d_i, d_j) # [B, 8, 15, 15]
    
    # 8í”½ì…€ ì „ì²´ì— ëŒ€í•œ ì—ëŸ¬ ì´í•©
    block_err = min_dist.sum(dim=1) # [B, 15, 15]
    
    # j <= i ì¡°ê±´ (jê°€ ië³´ë‹¤ í° ë¶€ë¶„ì€ ë¬´í•œëŒ€ ì²˜ë¦¬í•˜ì—¬ ë°°ì œ)
    mask = torch.tril(torch.ones(15, 15, dtype=torch.bool, device='cuda'))
    block_err = torch.where(mask, block_err, torch.tensor(float('inf'), device='cuda'))
    
    # ê° ë¸”ë¡(B)ì—ì„œ ê°€ì¥ ì—ëŸ¬ê°€ ì ì€ (fg, bg) ì¸ë±ìŠ¤ ë„ì¶œ
    flat_idx = block_err.view(num_blocks, -1).argmin(dim=1)
    best_i = flat_idx // 15
    best_j = flat_idx % 15
    
    # ê° ì¡°í•©ì— ë”°ë¥¸ PGT ë¹„íŠ¸ ê³„ì‚°
    best_di = dist[torch.arange(num_blocks), :, best_i] # [B, 8]
    best_dj = dist[torch.arange(num_blocks), :, best_j] # [B, 8]
    
    # d_i <= d_j ì¸ ê²½ìš° ë¹„íŠ¸ 1ë¡œ ì„¤ì • (ì „ê²½ìƒ‰)
    bit_mask = (best_di <= best_dj).int() # [B, 8]
    
    # 8ê°œì˜ ë¹„íŠ¸ë¥¼ í•˜ë‚˜ì˜ 1ë°”ì´íŠ¸ë¡œ ì••ì¶•
    shifts = torch.tensor([7, 6, 5, 4, 3, 2, 1, 0], dtype=torch.int32, device='cuda')
    p_byte = (bit_mask << shifts.unsqueeze(0)).sum(dim=1).to(torch.uint8) # [B]
    
    # ì‹¤ì œ MSX ì»¬ëŸ¬ì½”ë“œ 1-15ëŠ” ì¸ë±ìŠ¤ + 1
    fg = (best_i + 1).to(torch.uint8)
    bg = (best_j + 1).to(torch.uint8)
    c_byte = (fg << 4) | bg # [B]
    
    # GPU í…ì„œë¥¼ ë‹¤ì‹œ CPU í‰ë©´ ë°°ì—´ë¡œ ë³µì‚¬í•˜ì—¬ ì •ë ¬í•˜ê¸° 
    # [192, 32] ë¥¼ MSX VRAM ë©”ëª¨ë¦¬ ì£¼ì†Œ ìˆœì„œ (y//8 ë²ˆì§¸ ì¤„ì˜ xë¸”ë¡, y%8 ìŠ¤ìº”ë¼ì¸) ìœ¼ë¡œ ë§¤í•‘
    p_byte_cpu = p_byte.view(192, 32).cpu().numpy()
    c_byte_cpu = c_byte.view(192, 32).cpu().numpy()
    
    pgt = np.zeros(6144, dtype=np.uint8)
    ct = np.zeros(6144, dtype=np.uint8)
    
    for y in range(h):
        for cx in range(32):
            off = ((y // 8) * 32 + cx) * 8 + (y % 8)
            pgt[off] = p_byte_cpu[y, cx]
            ct[off] = c_byte_cpu[y, cx]
            
    return pgt, ct

@njit(fastmath=True, cache=True)
def _reconstruct_msx_frame(pgt, ct, pal_888_np):
    h, w = 192, 256
    out_img = np.zeros((h, w, 3), dtype=np.uint8)
    for y in range(h):
        for cx in range(32):
            x_start = cx * 8
            off = ((y // 8) * 32 + cx) * 8 + (y % 8)
            p_byte = pgt[off]
            c_byte = ct[off]
            fg = c_byte >> 4
            bg = c_byte & 0x0F
            
            for p in range(8):
                if (p_byte & (1 << (7 - p))) != 0:
                    out_img[y, x_start + p, 0] = pal_888_np[fg, 0]
                    out_img[y, x_start + p, 1] = pal_888_np[fg, 1]
                    out_img[y, x_start + p, 2] = pal_888_np[fg, 2]
                else:
                    out_img[y, x_start + p, 0] = pal_888_np[bg, 0]
                    out_img[y, x_start + p, 1] = pal_888_np[bg, 1]
                    out_img[y, x_start + p, 2] = pal_888_np[bg, 2]
    return out_img

# ==========================================================
# 2. ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í´ë˜ìŠ¤
# ==========================================================
def parse_time_str(t_str):
    if not t_str: return 0.0
    try:
        parts = str(t_str).split(':')
        if len(parts) == 3: return float(parts[0])*3600 + float(parts[1])*60 + float(parts[2])
        elif len(parts) == 2: return float(parts[0])*60 + float(parts[1])
        return float(parts[0])
    except ValueError: return 0.0

class MV2PerfectFrameEncoder:
    def __init__(self, input_video, output_mv2, quant_algo='kmeans', dither_mode='none', start_time=None, end_time=None, aspect_mode='pad', skip_prescale=False, use_temporal=False, debug_frames=False, scene_thresh=0.85, use_roi_face=False, use_roi_center=False, roi_center_spread=3.0, crop_up=0, crop_left=0, use_cuda=False):
        self.input_video = input_video
        self.output_mv2 = output_mv2
        self.quant_algo = quant_algo.lower()
        self.dither_mode = dither_mode.lower()
        self.aspect_mode = aspect_mode.lower()
        self.skip_prescale = skip_prescale
        self.start_sec = parse_time_str(start_time)
        self.end_sec = parse_time_str(end_time) if end_time else None
        
        self.use_temporal = use_temporal
        self.scene_thresh = scene_thresh  
        self.debug_frames = debug_frames 
        self.use_roi_face = use_roi_face
        self.use_roi_center = use_roi_center
        self.roi_center_spread = roi_center_spread
        self.crop_up = crop_up
        self.crop_left = crop_left
        self.use_cuda = use_cuda

        self.prev_hist = None
        self.prev_centroids = None
        
        # ğŸ’¡ [í•µì‹¬] OpenCVì— ë‚´ì¥ëœ Haar Cascade ì •ë©´ ì–¼êµ´ ì¸ì‹ ëª¨ë¸ ë¡œë“œ
        if self.use_roi_face:
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            if self.face_cascade.empty():
                print("[!] ê²½ê³ : OpenCV Haar Cascade ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ROI ê¸°ëŠ¥ì´ ë¬´ì‹œë©ë‹ˆë‹¤.")
                self.use_roi_face = False

        self.base_name = os.path.splitext(os.path.basename(input_video))[0]
        hash_str = hashlib.md5(f"{input_video}_{os.getpid()}".encode()).hexdigest()[:8]
        self.temp_mp3 = f"temp_audio_{self.base_name}_{hash_str}.mp3"
        self.temp_vid = f"temp_video_{self.base_name}_{hash_str}.mp4"

        if self.debug_frames:
            self.debug_dir = f"debug_frames_{self.base_name}"
            os.makedirs(self.debug_dir, exist_ok=True)
            print(f"[*] ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”: í”„ë ˆì„ ì´ë¯¸ì§€ê°€ '{self.debug_dir}' í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.")

    def _detect_scene_change(self, img_np):
        hist = cv2.calcHist([img_np], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
        cv2.normalize(hist, hist)
        
        is_scene_change = False
        if self.prev_hist is not None:
            score = cv2.compareHist(self.prev_hist, hist, cv2.HISTCMP_CORREL)
            if score < self.scene_thresh:
                is_scene_change = True
        else:
            is_scene_change = True
            
        self.prev_hist = hist
        return is_scene_change

    def _extract_palette(self, img_np, is_scene_change):
        n_colors = 15
        
        if self.quant_algo == 'kmeans':
            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
            
            # ğŸ’¡ [í•µì‹¬] í”½ì…€ ë³µì œ(ê°€ì¤‘ì¹˜)ë¥¼ ìœ„í•œ ë§ˆìŠ¤í¬ ë°°ì—´ ìƒì„± (ê¸°ë³¸ê°’: 1ë°°)
            weight_mask = np.ones(gray.shape, dtype=np.uint8)
            
            # 1. ìœ¤ê³½ì„  ê°€ì¤‘ì¹˜ (ê¸°ì¡´ 5ë°°ìˆ˜ ìœ ì§€)
            edges = cv2.Canny(gray, 50, 150)
            weight_mask[edges == 255] = 5
            
            # 2. ì–¼êµ´ ì¸ì‹ ROI ê°€ì¤‘ì¹˜ (ì••ë„ì ì¸ 30ë°°ìˆ˜ í• ë‹¹!)
            face_detected = False
            if self.use_roi_face:
                faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4, minSize=(30, 30))
                for (x, y, w, h) in faces:
                    # ì–¼êµ´ ì˜ì—­ì— í•´ë‹¹í•˜ëŠ” ë§ˆìŠ¤í¬ ë°°ì—´ ê°’ì„ 30ìœ¼ë¡œ ë®ì–´ì”Œì›€
                    weight_mask[y:y+h, x:x+w] = 30
                    face_detected = True

            # 3. ì¤‘ì•™ ì§‘ì¤‘ ROI ê°€ì¤‘ì¹˜ (ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ì¤‘ì•™ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
            if self.use_roi_center:
                h, w = gray.shape
                # Meshgrid 
                y, x = np.ogrid[:h, :w]
                center_y, center_x = h / 2, w / 2
                
                # ì •ê·œí™”ëœ 2D ê°€ìš°ì‹œì•ˆ ë§ˆìŠ¤í¬ (ì¤‘ì•™ 1.0, ì™¸ê³½ 0.0)
                # ì‹œê·¸ë§ˆ ì¡°ì •í•˜ì—¬ ì§‘ì¤‘ë„ ë³€ê²½ (ì‚¬ìš©ìê°€ ë„˜ê¸´ ìŠ¤í”„ë ˆë“œ ê³„ìˆ˜ë¡œ ë‚˜ëˆ„ê¸°)
                sigma_x, sigma_y = w / self.roi_center_spread, h / self.roi_center_spread
                gaussian_mask = np.exp(-(((x - center_x) ** 2) / (2 * sigma_x ** 2) + ((y - center_y) ** 2) / (2 * sigma_y ** 2)))
                
                # ê°•ë„ ì„¤ì •: ì¤‘ì•™ì€ ìµœëŒ€ 20ë°° ê°€ì¤‘ì¹˜, ì™¸ê³½ì€ ê¸°ë³¸ê°’ + Alpha
                roi_center_weight = (gaussian_mask * 20).astype(np.uint8)
                
                # ê¸°ì¡´ ë§ˆìŠ¤í¬(ì—£ì§€ë‚˜ ì–¼êµ´)ë¥¼ ë®ì–´ì“°ì§€ ì•Šê³  ê°€ì¥ í° ê°€ì¤‘ì¹˜ë¥¼ í•©ì‚°/ì´ˆì´ìŠ¤ 
                weight_mask = np.maximum(weight_mask, roi_center_weight)

            # 4. ë§ˆìŠ¤í¬(ê°€ì¤‘ì¹˜)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ í”½ì…€ ë°°ì—´ì„ ë¬¼ë¦¬ì ìœ¼ë¡œ ë³µì œ (Numpy ë§¤ì§)
            flat_img = img_np.reshape(-1, 3)
            flat_mask = weight_mask.reshape(-1)
            weighted_pixels = np.repeat(flat_img, flat_mask, axis=0)
            
            # ë©”ëª¨ë¦¬ í­ì£¼(OOM) ë°©ì§€: í”½ì…€ì´ ë„ˆë¬´ ë§ì•„ì§€ë©´ 30ë§Œ ê°œë¡œ ìƒ˜í”Œë§ (K-Means ì†ë„ ìœ ì§€)
            if len(weighted_pixels) > 300000:
                np.random.shuffle(weighted_pixels)
                weighted_pixels = weighted_pixels[:300000]
            
            unique_colors = len(np.unique(weighted_pixels, axis=0))
            if unique_colors < 1:
                raw = [(0,0,0)] * 15
                self.prev_centroids = None
            else:
                n_clusters = min(unique_colors, 15)
                
                if self.use_temporal and not is_scene_change and self.prev_centroids is not None and len(self.prev_centroids) == n_clusters:
                    init_val = np.array(self.prev_centroids)
                    n_init_val = 1 
                else:
                    init_val = 'k-means++'
                    n_init_val = 3 
                    
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    km = KMeans(n_clusters=n_clusters, init=init_val, n_init=n_init_val, max_iter=30).fit(weighted_pixels)
                    raw = [tuple(c) for c in km.cluster_centers_]
                    self.prev_centroids = raw.copy() 
                    
            return raw, face_detected # ğŸ’¡ ë””ë²„ê¹…ì„ ìœ„í•´ ì–¼êµ´ ê°ì§€ ì—¬ë¶€ ë°˜í™˜ ë³€ê²½

        else:
            pil_img = Image.fromarray(img_np)
            method = Image.Quantize.MEDIANCUT if self.quant_algo == 'mediancut' else Image.Quantize.FASTOCTREE
            quantized = pil_img.quantize(colors=n_colors, method=method)
            pal = quantized.getpalette()
            raw = [(pal[i], pal[i+1], pal[i+2]) for i in range(0, len(pal), 3)] if pal else []
            return raw, False

    def run(self):
        temporal_msg = f"í™œì„±í™” (ì„ê³„ê°’: {self.scene_thresh})" if self.use_temporal else "ë¹„í™œì„±í™”"
        roi_msg = "ì–¼êµ´ ì§‘ì¤‘(ROI 30x)" if self.use_roi_face else "ê¸°ë³¸"
        print(f"[*] ê³µì‹ ê·œê²© ì¸ì½”ë”© ì‹œì‘ (ì•Œê³ ë¦¬ì¦˜: {self.quant_algo.upper()}, ë””ë”: {self.dither_mode.upper()}, ì‹œê°„ì  ì¼ê´€ì„±: {temporal_msg}, ROI: {roi_msg})")
        
        time_args = []
        if self.start_sec > 0: time_args.extend(["-ss", str(self.start_sec)])
        if self.end_sec: time_args.extend(["-to", str(self.end_sec)])

        subprocess.run(["ffmpeg", "-y"] + time_args + ["-i", self.input_video, "-vn", "-acodec", "libmp3lame", "-ac", "2", "-ar", "44100", "-b:a", "128k", "-id3v2_version", "0", self.temp_mp3], capture_output=True)

        if not self.skip_prescale:
            print("[*] FFmpeg 512x384 ì‚¬ì „ ë Œë”ë§ ì¤‘...")
            
            # calculate pad/crop coordinates based on percent shift parameter (-100 to 100)
            # Default center formula: x=(ow-iw)/2, y=(oh-ih)/2
            # Modifier logic (e.g. crop_left=-100 pushes video full left, 100 pushes full right)
            x_shift = f"((ow-iw)/2)*(1.0+({self.crop_left}/100.0))"
            y_shift = f"((oh-ih)/2)*(1.0+({self.crop_up}/100.0))"
            cx_shift = f"((in_w-out_w)/2)*(1.0+({self.crop_left}/100.0))"
            cy_shift = f"((in_h-out_h)/2)*(1.0+({self.crop_up}/100.0))"

            if self.aspect_mode == 'pad': 
                vf_string = f"scale=512:384:force_original_aspect_ratio=decrease:flags=lanczos,pad=512:384:{x_shift}:{y_shift}:color=black"
            elif self.aspect_mode == 'crop': 
                vf_string = f"scale=512:384:force_original_aspect_ratio=increase:flags=lanczos,crop=512:384:{cx_shift}:{cy_shift}"
            else: 
                vf_string = "scale=512:384:flags=lanczos"
                
            input_args = ["-hwaccel", "cuda", "-i", self.input_video] if self.use_cuda else ["-i", self.input_video]
            codec_args = ["-c:v", "h264_nvenc", "-preset", "p1"] if self.use_cuda else ["-c:v", "libx264", "-preset", "ultrafast"]
            
            subprocess.run(["ffmpeg", "-y"] + time_args + input_args + ["-an", "-vf", vf_string, "-r", "15"] + codec_args + ["-crf", "10", self.temp_vid], capture_output=True)
            cap = cv2.VideoCapture(self.temp_vid)
            orig_fps = 15.0
        else:
            cap = cv2.VideoCapture(self.input_video)
            orig_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0

        with open(self.temp_mp3, "rb") as f: mp3_data = f.read()
        out_f = open(self.output_mv2, "wb")
        
        official_header = bytearray(16384)
        official_header[0:8] = b'MMCSD_MV'
        official_header[8:16] = b'        '
        official_header[16:21] = b'v2.00'
        out_f.write(official_header)

        idx, mp3_off, bps = 0, 0, 16000 
        
        while cap.isOpened():
            if self.skip_prescale:
                current_time = self.start_sec + (idx / 15.0)
                if self.end_sec and current_time > self.end_sec: break
                cap.set(cv2.CAP_PROP_POS_FRAMES, int(current_time * orig_fps))

            ret, frame = cap.read()
            if not ret or mp3_off >= len(mp3_data): break

            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img_512 = cv2.resize(img_rgb, (512, 384), interpolation=cv2.INTER_LANCZOS4) if self.skip_prescale else img_rgb
            
            is_scene_change = self._detect_scene_change(img_512)
            
            # ğŸ’¡ [ìˆ˜ì •] ì–¸íŒ¨í‚¹ ë¡œì§ ë³€ê²½ ë° íŒ”ë ˆíŠ¸ ì •ë ¬ ì ìš©
            raw_pal, face_detected = self._extract_palette(img_512, is_scene_change)
            
            final_pal_888 = raw_pal
            while len(final_pal_888) < 15: final_pal_888.append((0,0,0))
            final_pal_888 = final_pal_888[:15]
            final_pal_888.sort(key=lambda c: 0.299 * c[0] + 0.587 * c[1] + 0.114 * c[2])
            
            pal_333 = [tuple(int(round((c/255.0)*7)) for c in rgb) for rgb in final_pal_888]
            pal_888_np = np.zeros((16, 3), dtype=np.int32)
            for i, p in enumerate(pal_333):
                pal_888_np[i+1] = [int(c*255//7) for c in p]
            
            img_256 = cv2.resize(img_512, (256, 192), interpolation=cv2.INTER_AREA)
            
            if self.dither_mode == 'bayer':
                img_rgb_diffused = _apply_bayer_dither(img_256.astype(np.float32))
            elif self.dither_mode == 'bayer8':
                img_rgb_diffused = _apply_bayer8_dither(img_256.astype(np.float32))
            else:
                dither_flag = 2 if self.dither_mode == 'jjn' else (1 if self.dither_mode == 'fs' else 0)
                img_rgb_diffused = _apply_dither_rgb(img_256, pal_888_np, dither_flag)
            
            # ğŸ’¡ [í•µì‹¬] GPU ë³‘ë ¬ ì²˜ë¦¬ê°€ í™œì„±í™” ë˜ì–´ìˆëŠ”ì§€ ê²€ì‚¬ í›„ PyTorch ë¶„ê¸°, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê¸°ì¡´ Numba CPUë¡œ ë¶„ê¸°
            if self.use_cuda and HAS_TORCH and torch.cuda.is_available():
                pgt, ct = _encode_vram_optimal_search_cuda(img_rgb_diffused, pal_888_np)
            else:
                pgt, ct = _encode_vram_optimal_search(img_rgb_diffused, pal_888_np)

            if self.debug_frames:
                before_bgr = cv2.cvtColor(img_256, cv2.COLOR_RGB2BGR)
                cv2.imwrite(os.path.join(self.debug_dir, f"frame_{idx:04d}_before.png"), before_bgr)
                
                after_rgb = _reconstruct_msx_frame(pgt, ct, pal_888_np)
                after_bgr = cv2.cvtColor(after_rgb, cv2.COLOR_RGB2BGR)
                cv2.imwrite(os.path.join(self.debug_dir, f"frame_{idx:04d}_after.png"), after_bgr)
            
            pal_b = bytearray() 
            for r, g, b in pal_333: pal_b.extend([(r<<4)|b, g])
            if len(pal_b) < 30: pal_b.extend(b'\x00' * (30 - len(pal_b)))
            elif len(pal_b) > 30: pal_b = pal_b[:30]

            block = bytearray(b'\x55' * 16384) 
            block[0:6144] = pgt.tobytes()
            block[6144:12288] = ct.tobytes()
            block[12288:12318] = pal_b
            
            target_a = int((idx + 1) * (bps / 15))
            sz = max(1, min(111, math.ceil((target_a - mp3_off) / 32))) 
            block[12800] = sz
            chunk = mp3_data[mp3_off : mp3_off + sz*32]
            block[12801 : 12801+len(chunk)] = chunk
            mp3_off += len(chunk)
            
            out_f.write(block)
            
            status_char = "âœ‚ï¸ ì”¬ ì „í™˜!" if is_scene_change else ("ğŸ‘¤ ì–¼êµ´ ì§‘ì¤‘!" if face_detected else "  ")
            sys.stdout.write(f"\r  > {idx} í”„ë ˆì„ ì¸ì½”ë”© ì¤‘... {status_char}        ")
            sys.stdout.flush()
            idx += 1

        print("\n")
        eof = bytearray(16384); eof[12318] = 0x01; eof[12800] = 0x22 
        out_f.write(eof); cap.release(); out_f.close()
        
        if os.path.exists(self.temp_mp3): os.remove(self.temp_mp3)
        if os.path.exists(self.temp_vid): os.remove(self.temp_vid)
        print(f"[!] ê³µì‹ ê·œê²©(16KB í—¤ë”) ì™„ë²½ ì¸ì½”ë”© ì™„ë£Œ: {self.output_mv2}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MSX2 MV2 Perfect Frame Encoder (Math Optimal)")
    parser.add_argument("input", help="ì…ë ¥ ë™ì˜ìƒ íŒŒì¼ (.mp4)")
    parser.add_argument("output", help="ì¶œë ¥ ë™ì˜ìƒ íŒŒì¼ (.mv2)")
    parser.add_argument("--algo", choices=['kmeans', 'mediancut', 'octree'], default='kmeans', help="íŒ”ë ˆíŠ¸ ì–‘ìí™” ì•Œê³ ë¦¬ì¦˜")
    parser.add_argument("--dither", choices=['none', 'fs', 'jjn', 'bayer', 'bayer8'], default='none', help="ë””ë”ë§ ëª¨ë“œ")
    parser.add_argument("--temporal", action="store_true", help="[ì¶”ì²œ] ì”¬ ê°ì§€ë¥¼ í¬í•¨í•œ íŒ”ë ˆíŠ¸ ì‹œê°„ì  ì¼ê´€ì„±(ê¹œë¹¡ì„ ë°©ì§€) í™œì„±í™”")
    parser.add_argument("--scene-thresh", type=float, default=0.85, help="ì”¬ ì „í™˜ ê°ì§€ ì„ê³„ê°’ (ê¸°ë³¸: 0.85 / ì˜ˆë¯¼í•˜ê²Œ: 0.93)")
    
    # ğŸ’¡ [ì¶”ê°€] ì–¼êµ´ ì¸ì‹ ë° í™”ë©´ ì¤‘ì•™ ì§‘ì¤‘ íŒ¨í„´ (K-Means ì „ìš©)
    parser.add_argument("--roi-face", action="store_true", help="ì¸ë¬¼/ìºë¦­í„° ì–¼êµ´ì— íŒ”ë ˆíŠ¸ ìƒ‰ìƒì„ ëŒ€ê±° í• ë‹¹ (KMeans ì „ìš©)")
    parser.add_argument("--roi-center", action="store_true", help="í™”ë©´ ì¤‘ì•™ë¶€ì— íŒ”ë ˆíŠ¸ ìƒ‰ìƒì„ ì§‘ì¤‘ í• ë‹¹í•˜ëŠ” 2D ê°€ìš°ì‹œì•ˆ ROI íŒ¨í„´ ì ìš© (KMeans ì „ìš©)")
    parser.add_argument("--roi-center-spread", type=float, default=3.0, help="ì¤‘ì•™ ROI í¼ì§ ì •ë„ (ì‘ì„ìˆ˜ë¡ í™”ë©´ ì „ì²´ë¡œ ê· ë“±. ê¸°ë³¸: 3.0)")
    parser.add_argument("--cuda", action="store_true", help="NVIDIA CUDA(NVENC/NVDEC)ë¥¼ ì‚¬ìš©í•˜ì—¬ FFmpeg ë‹¤ìš´ìŠ¤ì¼€ì¼ ë Œë”ë§ì„ ë§¤ìš° ê°€ì†í™”í•©ë‹ˆë‹¤.")
    
    parser.add_argument("-ss", dest="start", default=None)
    parser.add_argument("-to", dest="end", default=None)
    parser.add_argument("--aspect", choices=['pad', 'crop', 'force'], default='pad')
    parser.add_argument("--skip-prescale", action="store_true")
    
    # ğŸ’¡ [ì¶”ê°€] ì—¬ë°± íŒ¨ìŠ¤ ë° í¬ë¡­ ìœ„ì¹˜ ì¡°ì ˆ (-100 ~ 100 í¼ì„¼íŠ¸ ë°°ì—´ ìŠ¤í¬ë¡¤)
    parser.add_argument("--crop-up", type=float, default=0, help="ë¹„ë””ì˜¤ ì¢…íš¡ë¹„ íŒ¨ë”©ì‹œ ìƒí•˜ ê°•ì œ ì´ë™ í¼ì„¼íŠ¸ (-100:ìƒë‹¨ ë”±ë¶™ ~ 100:í•˜ë‹¨ ë”±ë¶™)")
    parser.add_argument("--crop-left", type=float, default=0, help="ë¹„ë””ì˜¤ ì¢…íš¡ë¹„ íŒ¨ë”©ì‹œ ì¢Œìš° ê°•ì œ ì´ë™ í¼ì„¼íŠ¸ (-100:ì¢Œì¸¡ ë”±ë¶™ ~ 100:ìš°ì¸¡ ë”±ë¶™)")
    
    parser.add_argument("--debug-frame", "--debug-frames", dest="debug_frames", action="store_true", help="ì¸ì½”ë”© ì „/í›„ í”„ë ˆì„ì„ ì„ì‹œ í´ë”ì— ì €ì¥")
    
    args = parser.parse_args()
    
    MV2PerfectFrameEncoder(
        input_video=args.input, 
        output_mv2=args.output, 
        quant_algo=args.algo,
        dither_mode=args.dither, 
        start_time=args.start, 
        end_time=args.end, 
        aspect_mode=args.aspect, 
        skip_prescale=args.skip_prescale,
        use_temporal=args.temporal,
        debug_frames=args.debug_frames,
        scene_thresh=args.scene_thresh,
        use_roi_face=args.roi_face,
        use_roi_center=args.roi_center,
        roi_center_spread=args.roi_center_spread,
        crop_up=args.crop_up,
        crop_left=args.crop_left,
        use_cuda=args.cuda
        ).run()
